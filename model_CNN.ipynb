{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os, gc\n",
    "#regex libraries\n",
    "import re, regex\n",
    "\n",
    "#model libraries\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import hstack, csr_matrix, load_npz\n",
    "from itertools import combinations\n",
    "\n",
    "# keras-tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, concatenate, BatchNormalization, Flatten, Concatenate, Conv1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras_utils import KMaxPooling\n",
    "from keras.losses import mean_squared_error\n",
    "#peter's cache\n",
    "from cache import save_in_cache, load_cache\n",
    "\n",
    "from utils import rmse, print_step\n",
    "# timer function\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(mean_squared_error(y_true, y_pred)) \n",
    "    \n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = 'embeddings/avito_lookup_cc_ru_300.txt'\n",
    "data_dir = 'data'\n",
    "feat_dir = 'cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load data:] done in 12 s\n",
      "[Loading Lat-Lon & Region:] done in 2 s\n",
      "[bring in nima features] done in 5 s\n",
      "[preprocess dense features:] done in 2 s\n",
      "[bring in time features:] done in 1 s\n",
      "[Processing Numerical Features] done in 2 s\n",
      "[Processing Categorical Features] done in 37 s\n",
      "[preprocess text data:] done in 103 s\n",
      "[resize features:] done in 1 s\n",
      "[split data back to train and test:] done in 0 s\n",
      "Test shape: (508438, 302)\n",
      "Train shape: (1503424, 303)\n",
      "[2018-06-17 16:24:23.999907] Skipped... Loaded cache/train_img_data.csv and cache/test_img_data.csv from cache!\n",
      "[Loading Image data:] done in 60 s\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "with timer(\"load data:\"):\n",
    "    usecols = ['image_top_1','city','price','region','title','description','parent_category_name', 'user_type','category_name',\n",
    "              'item_seq_number', 'param_1', 'param_2', 'param_3', 'image']\n",
    "    train = pd.read_csv(f'{data_dir}/train.csv', usecols=usecols+['deal_probability'])\n",
    "    test = pd.read_csv(f'{data_dir}/test.csv', usecols=usecols)\n",
    "\n",
    "    train_split = len(train)\n",
    "    y = train['deal_probability'].copy()\n",
    "    train.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.concat([train,test],axis=0)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "with timer(\"Loading Lat-Lon & Region:\"):\n",
    "    # Lat Lon\n",
    "    df['location'] = df['city'] + ', ' + df['region']\n",
    "    locations = pd.read_csv('city_latlons.csv')\n",
    "    df = df.merge(locations, how='left', on='location')\n",
    "    df.drop('location', axis=1, inplace=True)\n",
    "    ## Region Macro\n",
    "    region_macro = pd.read_csv('region_macro.csv')\n",
    "    df = df.merge(region_macro, how='left', on='region')\n",
    "    for c in ['lat', 'lon', 'unemployment_rate', 'GDP_PC_PPP', 'HDI']:\n",
    "        df[c] = (df[c] - np.mean(df[c]))/np.std(df[c])\n",
    "\n",
    "with timer('bring in nima features'):\n",
    "    train_img_nima = pd.read_csv(f'{feat_dir}/train_img_nima.csv')\n",
    "    test_img_nima = pd.read_csv(f'{feat_dir}/test_img_nima.csv')\n",
    "\n",
    "    df_img_nima = pd.concat([train_img_nima,test_img_nima],axis=0)\n",
    "\n",
    "    df = df.merge(df_img_nima, on = 'image', how = 'left')\n",
    "    df.drop(['image'], axis=1, inplace=True)\n",
    "    nima_cols = [\"mobile_mean\", \"mobile_std\",\"inception_mean\", \"inception_std\", \"nasnet_mean\", \"nasnet_std\"]\n",
    "    df[nima_cols] = df[nima_cols].fillna(0)\n",
    "    for c in nima_cols:\n",
    "        df[c] = (df[c] - np.mean(df[c]))/np.std(df[c])\n",
    "        \n",
    "# pre-processing\n",
    "with timer(\"preprocess dense features:\"):\n",
    "       \n",
    "    # merge params\n",
    "    param_cols = ['param_1', 'param_2', 'param_3']\n",
    "    for c in param_cols:\n",
    "        df[c] = df[c].astype(str)\n",
    "        df[c] = df[c].fillna(value='missing')\n",
    "\n",
    "    df['param123'] = (df['param_1']+'_'+df['param_2']+'_'+df['param_3']).astype(str)\n",
    "    df.drop(['param_2','param_3'], axis=1, inplace=True)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"bring in time features:\"):\n",
    "    train_time = pd.read_csv(f'{feat_dir}/train_time.csv')\n",
    "    test_time = pd.read_csv(f'{feat_dir}/test_time.csv')\n",
    "    df_time = pd.concat([train_time, test_time], axis=0).reset_index(drop = True)\n",
    "    df = pd.concat([df, df_time], axis=1)\n",
    "    \n",
    "with timer('Processing Numerical Features'):\n",
    "    # price - log transform & fill NA\n",
    "    num_cols = ['price', 'item_seq_number','days_up_user_mean','times_up_user_mean','days_up_user_median',\n",
    "                'times_up_user_median','days_up_user_min','times_up_user_min','days_up_user_max','times_up_user_max',\n",
    "                'n_user_items']\n",
    "    \n",
    "    for c in num_cols:\n",
    "        df[c+'_missing'] = 0\n",
    "        df[c] = df[c].replace([-999], np.NaN) # remove previously imputed NaN (-999)\n",
    "        df[c+'_missing'] = np.where(df[c].isnull(), 1, df[c+'_missing'])\n",
    "        df[c] = df[c].replace([np.NaN], 0)\n",
    "        df[c] = np.log1p(df[c])\n",
    "        df[c] = (df[c] - np.mean(df[c]))/np.std(df[c])\n",
    "    df.drop(['item_seq_number_missing','n_user_items_missing'], axis=1, inplace=True)\n",
    "    \n",
    "    ## add in previously processed features\n",
    "    num_cols += nima_cols\n",
    "    num_cols += ['lat', 'lon', 'unemployment_rate', 'GDP_PC_PPP', 'HDI']\n",
    "    \n",
    "    miss_cols = [col for col in df.columns if '_missing' in col]\n",
    "\n",
    "with timer('Processing Categorical Features'):\n",
    "    cat_cols = ['image_top_1','city','region','parent_category_name','user_type','category_name','param123','param_1']\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].astype(str)\n",
    "        df[c].fillna(value='missing', inplace=True)\n",
    "        df[c] = df[c].str.lower()\n",
    "        df[c] = df[c].replace(to_replace=' +', value=' ', regex=True)\n",
    "\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    for c in cat_cols:\n",
    "        df[c] = lbl.fit_transform(df[c])\n",
    "        \n",
    "    # print shape\n",
    "    cat_cols = cat_cols+miss_cols\n",
    "    gc.collect()\n",
    "\n",
    "with timer(\"preprocess text data:\"):\n",
    "    def clean_text(text):\n",
    "        text = bytes(text, encoding=\"utf-8\")\n",
    "        text = text.lower()\n",
    "        text = re.sub(b'(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', b' ', text)\n",
    "        text = re.sub(b'\\s+(?=\\d)|(?<=\\d)\\s+', b' ', text)\n",
    "        text = text.replace(b\"\\b\", b\" \")\n",
    "        text = text.replace(b\"\\r\", b\" \")\n",
    "        text = regex.sub(b\"\\s+\", b\" \", text)\n",
    "        text = str(text, 'utf-8')\n",
    "        text = re.sub(r\"\\W+\", \" \", text.lower())\n",
    "        return text\n",
    "    \n",
    "    text_input = df['title'].str.cat([\n",
    "        df['description']], sep=' ', na_rep='').astype(str).fillna('missing')\n",
    "    \n",
    "    text_output = [clean_text(x) for x in text_input]\n",
    "    \n",
    "    df.drop(['description','title'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"resize features:\"):\n",
    "    # reduce size\n",
    "    for c in cat_cols:\n",
    "        if df[c].max()<2**7:\n",
    "            df[c] = df[c].astype('int8')\n",
    "        elif df[c].max()<2**15:\n",
    "            df[c] = df[c].astype('int16')\n",
    "        elif df[c].max()<2**31:\n",
    "            df[c] = df[c].astype('int32')\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "with timer(\"split data back to train and test:\"):\n",
    "    df_train = df[:train_split]\n",
    "    df_test = df[train_split:]\n",
    "    \n",
    "    text_train = text_output[:train_split]\n",
    "    text_test = text_output[train_split:]\n",
    "\n",
    "    del df\n",
    "\n",
    "    # get max cat size\n",
    "    emb_cat_max = {}\n",
    "    for c in cat_cols:\n",
    "        emb_cat_max[c] = max(df_train[c].max(), df_test[c].max())+1\n",
    "    \n",
    "    # get embedding cat size\n",
    "    emb_cat_size = {}\n",
    "    for c in cat_cols:\n",
    "        emb_cat_size[c] = int(min(50, pd.concat([df_train[c], df_test[c]], axis=0).nunique() / 2))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"Loading Image data:\"):\n",
    "    train_img, test_img = load_cache('img_data')\n",
    "    cols = ['img_size_x', 'img_size_y', 'img_file_size', 'img_mean_color', 'img_dullness_light_percent', 'img_dullness_dark_percent', 'img_blur', 'img_blue_mean', 'img_green_mean', 'img_red_mean', 'img_blue_std', 'img_green_std', 'img_red_std', 'img_average_red', 'img_average_green', 'img_average_blue', 'img_sobel00', 'img_sobel10', 'img_sobel20', 'img_sobel01', 'img_sobel11', 'img_sobel21', 'img_kurtosis', 'img_skew', 'thing1', 'thing2']\n",
    "    train_img[cols] = train_img[cols].fillna(0)\n",
    "    test_img[cols] = test_img[cols].fillna(0)\n",
    "    img = pd.concat([train_img[cols],test_img[cols]],axis=0)\n",
    "    for c in cols:\n",
    "        img[c] = (img[c] - np.mean(img[c]))/np.std(img[c])\n",
    "    train_img = img[:train_split]\n",
    "    test_img = img[train_split:]\n",
    "    df_train = pd.concat([df_train, train_img], axis=1)\n",
    "    df_test = pd.concat([df_test.reset_index(drop = True), test_img], axis=1)\n",
    "    num_cols += cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tokenize text data:] done in 65 s\n",
      "Unique Words: found 793000 out of total 793308 words at a rate of 99.96%\n",
      "[prepare embeddings:] done in 196 s\n",
      "[prepare text input:] done in 61 s\n"
     ]
    }
   ],
   "source": [
    "# prepare embeddings\n",
    "embed_size = 300\n",
    "maxlen = 250\n",
    "\n",
    "with timer(\"tokenize text data:\"):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_output)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "with timer(\"prepare embeddings:\"):\n",
    "    # embed_size = 300\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(embed_file, encoding=\"utf8\"))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    nb_words = len(word_index) + 1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            hit += 1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        total += 1\n",
    "    print(\"Unique Words: found {} out of total {} words at a rate of {:.2f}%\".format(hit, total, hit * 100.0 / total))\n",
    "    del embeddings_index, embedding_vector\n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"prepare text input:\"):\n",
    "    text_train_token = tokenizer.texts_to_sequences(text_train)\n",
    "    text_train = pad_sequences(text_train_token, maxlen=maxlen)\n",
    "    \n",
    "    text_test_token = tokenizer.texts_to_sequences(text_test)\n",
    "    text_test = pad_sequences(text_test_token, maxlen=maxlen)\n",
    "    del text_output, text_train_token, text_test_token\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "def get_keras_data(df, text):\n",
    "    X = {}\n",
    "    for c in df.columns:\n",
    "        X[c] = df[c].values\n",
    "    X['text'] = text\n",
    "    return X\n",
    "\n",
    "def runCNN(train_X, train_y, val_X, val_y, test_X):\n",
    "    model = CNN()\n",
    "    \n",
    "    ### Callbacks\n",
    "    def schedule(ind):\n",
    "        return(1.5e-3/(1.5**(ind)))\n",
    "\n",
    "    lr = LearningRateScheduler(schedule)\n",
    "    \n",
    "#     tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "#                           write_graph=True, write_images=False)\n",
    "    \n",
    "#     early_stopping = EarlyStopping(\n",
    "#         patience=2,\n",
    "#         verbose=1,\n",
    "#         min_delta=1e-4,\n",
    "#         monitor='val_mean_squared_error'\n",
    "#         )\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(filepath = 'tmp_bst_mdl.hdf5', monitor='val_mean_squared_error', verbose=0, save_best_only = True, mode='min')\n",
    "\n",
    "    tb = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    model.fit(train_X, train_y, \n",
    "              validation_data=(val_X, val_y),\n",
    "              batch_size=256, epochs=5, verbose=2,\n",
    "             callbacks = [lr, tb, model_checkpoint])\n",
    "    print_step(\"Loading best model\")\n",
    "    model.load_weights(filepath = 'tmp_bst_mdl.hdf5')\n",
    "    print_step('Predict Val 1/2')\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    print_step('Predict Test 2/2')\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_val_y, pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(): \n",
    "        \n",
    "    K.clear_session()\n",
    "\n",
    "    cats = [Input(shape=[1], name=name) for name in cat_cols]\n",
    "    nums = [Input(shape=[1], name=name) for name in num_cols]\n",
    "\n",
    "    emb_fn = lambda name: Embedding(emb_cat_max[name], emb_cat_size[name])\n",
    "    embs = []\n",
    "    for name, cat in zip(cat_cols, cats):\n",
    "        embs.append(emb_fn(name)(cat))\n",
    "\n",
    "    texts = Input(shape=(maxlen, ), name='text')\n",
    "\n",
    "    text_emb = Embedding(nb_words, \n",
    "                         embed_size,\n",
    "                         trainable=False,\n",
    "                         weights=[embedding_matrix],\n",
    "                         name='text_emb')(texts)\n",
    "    outs = []\n",
    "\n",
    "    filter_sizes = [1,2,3,4]\n",
    "    convs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(text_emb)\n",
    "        l_pool = KMaxPooling(k=30, axis=1)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "    l_flat = Flatten()(l_merge)\n",
    "    conv_in = Dense(64, activation='relu')(l_flat)\n",
    "\n",
    "    outs += [conv_in]\n",
    "\n",
    "    # bring in embeddings        \n",
    "    all_in = [Flatten()(emb) for emb in embs] + nums\n",
    "    x_in = concatenate(all_in)\n",
    "\n",
    "#     for idx, (drop_p, num_dense) in enumerate(zip([0.2, 0.2], [64, 32])):\n",
    "    for num_dense in [128, 64]:\n",
    "        x_in = Dense(num_dense, activation='relu')(x_in)\n",
    "#         x_in = Dropout(drop_p)(x_in)\n",
    "\n",
    "    deep = x_in\n",
    "\n",
    "    outs += [deep]\n",
    "\n",
    "    output = concatenate(outs) if len(outs)>1 else outs[0]\n",
    "    \n",
    "\n",
    "#    output = Dropout(0.2)(output)\n",
    "\n",
    "    output = Dense(128, activation='relu')(output)\n",
    "#     l_dense = Dropout(0.2)(l_dense)\n",
    "#     output = Dense(32, activation='relu')(output)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model(inputs=cats+nums+[texts], outputs=output)\n",
    "    model.compile(loss = 'mse',\n",
    "                  metrics=['mse'],\n",
    "                  optimizer='nadam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_model(train, text_train, test, text_test, target, model_fn, eval_fn):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    fold_splits = kf.split(train)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros(train.shape[0])\n",
    "    i = 1\n",
    "    test_X = get_keras_data(test, text_test)\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print_step('Started ' + ' fold ' + str(i) + '/5')\n",
    "        dev_X = get_keras_data(train.loc[dev_index], text_train[dev_index])\n",
    "        val_X = get_keras_data(train.loc[val_index], text_train[val_index])\n",
    "        dev_y, val_y = target[dev_index], target[val_index]\n",
    "        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test_X)\n",
    "        pred_val_y = pred_val_y.flatten()\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        cv_score = eval_fn(val_y, pred_val_y)\n",
    "        cv_scores.append(eval_fn(val_y, pred_val_y))\n",
    "        print_step(' cv score ' + str(i) + ' : ' + str(cv_score))\n",
    "        i += 1\n",
    "    print_step(' cv scores : ' + str(cv_scores))\n",
    "    print_step(' mean cv score : ' + str(np.mean(cv_scores)))\n",
    "    print_step(' std cv score : ' + str(np.std(cv_scores)))\n",
    "    pred_full_test = pred_full_test / 5.0\n",
    "    results = {'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-17 16:29:47.410498] Started  fold 1/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "results = run_cv_model(df_train, text_train, df_test, text_test, y, runCNN, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters = [2,3,4] BN dense feature, batch 512\n",
    "# 0.2218690800503754\n",
    "\n",
    "# filters = [1,2,3,4] no BN, batch 512, output dense [128]\n",
    "# 0.2209\n",
    "\n",
    "# filters = [2,3,4] no BN, batch 512, output dense [128]\n",
    "# 0.2212\n",
    "\n",
    "# filters = [1,2,3,4] no BN, batch 128, output dense [128, 32]\n",
    "# 0.22087\n",
    "\n",
    "# filters = [1,2,3,4] no BN, batch 128, output dense [128], + img data\n",
    "# 0.2203\n",
    "\n",
    "# filters = [1,2,3,4] no BN, batch 128, output dense [128], + img data + lat lon + region macro\n",
    "# 0.22026\n",
    "\n",
    "# numerical feature [64, 32]\n",
    "# 0.21965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[2018-06-17 16:01:43.989447] Prepping submission file\n",
      "[2018-06-17 16:01:45.664488] Done!\n"
     ]
    }
   ],
   "source": [
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print_step('Prepping submission file')\n",
    "submission = pd.read_csv(f'{data_dir}/test.csv', usecols=['item_id'])\n",
    "submission['deal_probability'] = results['test'].clip(0.0, 1.0)\n",
    "submission.to_csv('submit/submit_CNN_FastText_3.csv', index=False)\n",
    "print_step('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~\n",
      "[2018-06-17 16:01:50.468442] Cache\n",
      "[2018-06-17 16:01:53.179384] Saved cache/train_CNN_FastText_2.csv and cache/test_CNN_FastText_2.csv to cache!\n"
     ]
    }
   ],
   "source": [
    "print('~~~~~~~~~~')\n",
    "print_step('Cache')\n",
    "save_in_cache('CNN_FastText_2', pd.DataFrame({'CNN_FastText_3': results['train']}),\n",
    "                           pd.DataFrame({'CNN_FastText_3': results['test'].flatten()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijunhe/Environment/ML_py_36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os, gc\n",
    "#regex libraries\n",
    "import re, regex\n",
    "\n",
    "#model libraries\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import hstack, csr_matrix, load_npz\n",
    "from itertools import combinations\n",
    "\n",
    "# keras-tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, concatenate, BatchNormalization, Flatten, Concatenate, Conv1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras_utils import KMaxPooling\n",
    "from keras.losses import mean_squared_error\n",
    "#peter's cache\n",
    "from cache import save_in_cache, load_cache\n",
    "\n",
    "from utils import rmse, print_step\n",
    "# timer function\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(mean_squared_error(y_true, y_pred)) \n",
    "    \n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_file = 'embeddings/avito_lookup_cc_ru_300.txt'\n",
    "data_dir = 'data'\n",
    "feat_dir = 'cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load data:] done in 12 s\n",
      "[Loading Lat-Lon & Region:] done in 2 s\n",
      "[bring in nima features] done in 17 s\n",
      "[preprocess dense features:] done in 2 s\n",
      "[bring in time features:] done in 1 s\n",
      "[Processing Numerical Features] done in 2 s\n",
      "[preprocess text data:] done in 107 s\n",
      "[Processing Categorical Features] done in 36 s\n",
      "[resize features:] done in 0 s\n",
      "[split data back to train and test:] done in 0 s\n",
      "Test shape: (508438, 302)\n",
      "Train shape: (1503424, 303)\n",
      "[2018-06-24 15:10:08.958821] Skipped... Loaded cache/train_img_data.csv and cache/test_img_data.csv from cache!\n",
      "[Loading Image data:] done in 61 s\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "with timer(\"load data:\"):\n",
    "    usecols = ['image_top_1','city','price','region','title','description','parent_category_name', 'user_type','category_name',\n",
    "              'item_seq_number', 'param_1', 'param_2', 'param_3', 'image']\n",
    "    train = pd.read_csv(f'{data_dir}/train.csv', usecols=usecols+['deal_probability'])\n",
    "    test = pd.read_csv(f'{data_dir}/test.csv', usecols=usecols)\n",
    "\n",
    "    train_split = len(train)\n",
    "    y = train['deal_probability'].copy()\n",
    "    train.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "    \n",
    "    df = pd.concat([train,test],axis=0)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "with timer(\"Loading Lat-Lon & Region:\"):\n",
    "    # Lat Lon\n",
    "    df['location'] = df['city'] + ', ' + df['region']\n",
    "    locations = pd.read_csv('city_latlons.csv')\n",
    "    df = df.merge(locations, how='left', on='location')\n",
    "    df.drop('location', axis=1, inplace=True)\n",
    "    ## Region Macro\n",
    "    region_macro = pd.read_csv('region_macro.csv')\n",
    "    df = df.merge(region_macro, how='left', on='region')\n",
    "    for c in ['lat', 'lon', 'unemployment_rate', 'GDP_PC_PPP', 'HDI']:\n",
    "        df[c] = (df[c] - np.mean(df[c]))/np.std(df[c])\n",
    "\n",
    "with timer('bring in nima features'):\n",
    "    train_img_nima = pd.read_csv(f'{feat_dir}/train_img_nima.csv')\n",
    "    test_img_nima = pd.read_csv(f'{feat_dir}/test_img_nima.csv')\n",
    "    df_img_nima = pd.concat([train_img_nima,test_img_nima],axis=0)\n",
    "    train_img_nima_softmax = pd.read_csv(f'{feat_dir}/train_img_nima_softmax.csv')\n",
    "    test_img_nima_softmax = pd.read_csv(f'{feat_dir}/test_img_nima_softmax.csv')\n",
    "    df_img_nima_softmax = pd.concat([train_img_nima_softmax,test_img_nima_softmax],axis=0)\n",
    "    df = df.merge(df_img_nima, on = 'image', how = 'left').merge(df_img_nima_softmax, on = 'image', how = 'left')\n",
    "    df.drop(['image'], axis=1, inplace=True)\n",
    "    nima_cols = list(np.setdiff1d(df_img_nima.columns, ['image'])) + list(np.setdiff1d(df_img_nima_softmax.columns, ['image']))\n",
    "    df[nima_cols] = df[nima_cols].fillna(0)\n",
    "    for c in nima_cols:\n",
    "        df[c] = (df[c] - np.mean(df[c]))/np.std(df[c])\n",
    "        \n",
    "# pre-processing\n",
    "with timer(\"preprocess dense features:\"):\n",
    "       \n",
    "    # merge params\n",
    "    param_cols = ['param_1', 'param_2', 'param_3']\n",
    "    for c in param_cols:\n",
    "        df[c] = df[c].astype(str)\n",
    "        df[c] = df[c].fillna(value='missing')\n",
    "\n",
    "    df['param123'] = (df['param_1']+'_'+df['param_2']+'_'+df['param_3']).astype(str)\n",
    "    df.drop(['param_2','param_3'], axis=1, inplace=True)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"bring in time features:\"):\n",
    "    train_time = pd.read_csv(f'{feat_dir}/train_time.csv')\n",
    "    test_time = pd.read_csv(f'{feat_dir}/test_time.csv')\n",
    "    df_time = pd.concat([train_time, test_time], axis=0).reset_index(drop = True)\n",
    "    df = pd.concat([df, df_time], axis=1)\n",
    "    \n",
    "with timer('Processing Numerical Features'):\n",
    "    # price - log transform & fill NA\n",
    "    num_cols = ['price', 'item_seq_number','days_up_user_mean','times_up_user_mean','days_up_user_median',\n",
    "                'times_up_user_median','days_up_user_min','times_up_user_min','days_up_user_max','times_up_user_max',\n",
    "                'n_user_items']\n",
    "    \n",
    "    for c in num_cols:\n",
    "        df[c+'_missing'] = 0\n",
    "        df[c] = df[c].replace([-999], np.NaN) # remove previously imputed NaN (-999)\n",
    "        df[c+'_missing'] = np.where(df[c].isnull(), 1, df[c+'_missing'])\n",
    "        df[c] = df[c].replace([np.NaN], 0)\n",
    "        df[c] = np.log1p(df[c])\n",
    "        df[c] = (df[c] - np.mean(df[c]))/np.std(df[c])\n",
    "    df.drop(['item_seq_number_missing','n_user_items_missing'], axis=1, inplace=True)\n",
    "    \n",
    "    ## add in previously processed features\n",
    "    num_cols += nima_cols\n",
    "    num_cols += ['lat', 'lon', 'unemployment_rate', 'GDP_PC_PPP', 'HDI']\n",
    "    \n",
    "    miss_cols = [col for col in df.columns if '_missing' in col]\n",
    "\n",
    "with timer(\"preprocess text data:\"):\n",
    "    def clean_text(text):\n",
    "        text = bytes(text, encoding=\"utf-8\")\n",
    "        text = text.lower()\n",
    "        text = re.sub(b'(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', b' ', text)\n",
    "        text = re.sub(b'\\s+(?=\\d)|(?<=\\d)\\s+', b' ', text)\n",
    "        text = text.replace(b\"\\b\", b\" \")\n",
    "        text = text.replace(b\"\\r\", b\" \")\n",
    "        text = regex.sub(b\"\\s+\", b\" \", text)\n",
    "        text = str(text, 'utf-8')\n",
    "        text = re.sub(r\"\\W+\", \" \", text.lower())\n",
    "        return text\n",
    "    \n",
    "    text_input = df['title'].str.cat([\n",
    "        df['description'], df['parent_category_name']], sep=' ', na_rep='').astype(str).fillna('missing')\n",
    "    \n",
    "    text_output = [clean_text(x) for x in text_input]\n",
    "    \n",
    "    df.drop(['description','title'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "with timer('Processing Categorical Features'):\n",
    "    cat_cols = ['image_top_1', 'city','region','parent_category_name','user_type','category_name','param123','param_1']\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].astype(str)\n",
    "        df[c].fillna(value='missing', inplace=True)\n",
    "        df[c] = df[c].str.lower()\n",
    "        df[c] = df[c].replace(to_replace=' +', value=' ', regex=True)\n",
    "\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    for c in cat_cols:\n",
    "        df[c] = lbl.fit_transform(df[c])\n",
    "    \n",
    "with timer(\"resize features:\"):\n",
    "    # reduce size\n",
    "    for c in cat_cols:\n",
    "        if df[c].max()<2**7:\n",
    "            df[c] = df[c].astype('int8')\n",
    "        elif df[c].max()<2**15:\n",
    "            df[c] = df[c].astype('int16')\n",
    "        elif df[c].max()<2**31:\n",
    "            df[c] = df[c].astype('int32')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "            cat_cols = cat_cols+miss_cols\n",
    "            \n",
    "with timer(\"split data back to train and test:\"):\n",
    "    df_train = df[:train_split]\n",
    "    df_test = df[train_split:]\n",
    "    \n",
    "    text_train = text_output[:train_split]\n",
    "    text_test = text_output[train_split:]\n",
    "\n",
    "    del df\n",
    "\n",
    "    # get max cat size\n",
    "    emb_cat_max = {}\n",
    "    for c in cat_cols:\n",
    "        emb_cat_max[c] = max(df_train[c].max(), df_test[c].max())+1\n",
    "    \n",
    "    # get embedding cat size\n",
    "    emb_cat_size = {}\n",
    "    for c in cat_cols:\n",
    "        emb_cat_size[c] = int(min(50, pd.concat([df_train[c], df_test[c]], axis=0).nunique() / 2))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"Loading Image data:\"):\n",
    "    train_img, test_img = load_cache('img_data')\n",
    "    cols = ['img_size_x', 'img_size_y', 'img_file_size', 'img_mean_color', 'img_dullness_light_percent', 'img_dullness_dark_percent', 'img_blur', 'img_blue_mean', 'img_green_mean', 'img_red_mean', 'img_blue_std', 'img_green_std', 'img_red_std', 'img_average_red', 'img_average_green', 'img_average_blue', 'img_sobel00', 'img_sobel10', 'img_sobel20', 'img_sobel01', 'img_sobel11', 'img_sobel21', 'img_kurtosis', 'img_skew', 'thing1', 'thing2']\n",
    "    train_img[cols] = train_img[cols].fillna(0)\n",
    "    test_img[cols] = test_img[cols].fillna(0)\n",
    "    img = pd.concat([train_img[cols],test_img[cols]],axis=0)\n",
    "    for c in cols:\n",
    "        img[c] = (img[c] - np.mean(img[c]))/np.std(img[c])\n",
    "    train_img = img[:train_split]\n",
    "    test_img = img[train_split:]\n",
    "    df_train = pd.concat([df_train, train_img], axis=1)\n",
    "    df_test = pd.concat([df_test.reset_index(drop = True), test_img], axis=1)\n",
    "    num_cols += cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tokenize text data:] done in 66 s\n",
      "Unique Words: found 793000 out of total 793308 words at a rate of 99.96%\n",
      "[prepare embeddings:] done in 194 s\n",
      "[prepare text input:] done in 62 s\n"
     ]
    }
   ],
   "source": [
    "# prepare embeddings\n",
    "embed_size = 300\n",
    "maxlen = 250\n",
    "\n",
    "with timer(\"tokenize text data:\"):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_output)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "with timer(\"prepare embeddings:\"):\n",
    "    # embed_size = 300\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(embed_file, encoding=\"utf8\"))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    nb_words = len(word_index) + 1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            hit += 1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        total += 1\n",
    "    print(\"Unique Words: found {} out of total {} words at a rate of {:.2f}%\".format(hit, total, hit * 100.0 / total))\n",
    "    del embeddings_index, embedding_vector\n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"prepare text input:\"):\n",
    "    text_train_token = tokenizer.texts_to_sequences(text_train)\n",
    "    text_train = pad_sequences(text_train_token, maxlen=maxlen)\n",
    "    \n",
    "    text_test_token = tokenizer.texts_to_sequences(text_test)\n",
    "    text_test = pad_sequences(text_test_token, maxlen=maxlen)\n",
    "    del text_output, text_train_token, text_test_token\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "def get_keras_data(df, text):\n",
    "    X = {}\n",
    "    for c in df.columns:\n",
    "        X[c] = df[c].values\n",
    "    X['text'] = text\n",
    "    return X\n",
    "\n",
    "def runCNN(train_X, train_y, val_X, val_y, test_X):\n",
    "    model = CNN()\n",
    "    \n",
    "    ### Callbacks\n",
    "    def schedule(ind):\n",
    "        return(1.5e-3/(1.5**(ind)))\n",
    "\n",
    "    lr = LearningRateScheduler(schedule)\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(filepath = 'tmp_bst_mdl.hdf5', monitor='val_mean_squared_error', verbose=0, save_best_only = True, mode='min')\n",
    "\n",
    "    tb = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    model.fit(train_X, train_y, \n",
    "              validation_data=(val_X, val_y),\n",
    "              batch_size=256, epochs=4, verbose=2,\n",
    "             callbacks = [lr, model_checkpoint])\n",
    "    print_step(\"Loading best model\")\n",
    "    model.load_weights(filepath = 'tmp_bst_mdl.hdf5')\n",
    "    print_step('Predict Val 1/2')\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    print_step('Predict Test 2/2')\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_val_y, pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(): \n",
    "        \n",
    "    K.clear_session()\n",
    "\n",
    "    cats = [Input(shape=[1], name=name) for name in cat_cols]\n",
    "    nums = [Input(shape=[1], name=name) for name in num_cols]\n",
    "\n",
    "    emb_fn = lambda name: Embedding(emb_cat_max[name], emb_cat_size[name])\n",
    "    embs = []\n",
    "    for name, cat in zip(cat_cols, cats):\n",
    "        embs.append(emb_fn(name)(cat))\n",
    "\n",
    "    texts = Input(shape=(maxlen, ), name='text')\n",
    "\n",
    "    text_emb = Embedding(nb_words, \n",
    "                         embed_size,\n",
    "                         trainable=False,\n",
    "                         weights=[embedding_matrix],\n",
    "                         name='text_emb')(texts)\n",
    "    outs = []\n",
    "\n",
    "    filter_sizes = [1,2,3,4]\n",
    "    convs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(text_emb)\n",
    "        l_pool = KMaxPooling(k=20, axis=1)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "    l_flat = Flatten()(l_merge)\n",
    "    conv_in = Dense(64, activation='relu')(l_flat)\n",
    "\n",
    "    outs += [conv_in]\n",
    "\n",
    "    # bring in embeddings        \n",
    "    all_in = [Flatten()(emb) for emb in embs] + nums\n",
    "    x_in = concatenate(all_in)\n",
    "\n",
    "#     for idx, (drop_p, num_dense) in enumerate(zip([0.2, 0.2], [64, 32])):\n",
    "    for num_dense in [192, 96]:\n",
    "        x_in = Dense(num_dense, activation='relu')(x_in)\n",
    "#         x_in = Dropout(drop_p)(x_in)\n",
    "\n",
    "    deep = x_in\n",
    "\n",
    "    outs += [deep]\n",
    "\n",
    "    output = concatenate(outs) if len(outs)>1 else outs[0]\n",
    "    \n",
    "\n",
    "#    output = Dropout(0.2)(output)\n",
    "\n",
    "    output = Dense(64, activation='relu')(output)\n",
    "#     l_dense = Dropout(0.2)(l_dense)\n",
    "#     output = Dense(32, activation='relu')(output)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model(inputs=cats+nums+[texts], outputs=output)\n",
    "    model.compile(loss = 'mse',\n",
    "                  metrics=['mse'],\n",
    "                  optimizer='nadam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_model(train, text_train, test, text_test, target, model_fn, eval_fn):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    fold_splits = kf.split(train)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros(train.shape[0])\n",
    "    i = 1\n",
    "    test_X = get_keras_data(test, text_test)\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print_step('Started ' + ' fold ' + str(i) + '/5')\n",
    "        dev_X = get_keras_data(train.loc[dev_index], text_train[dev_index])\n",
    "        val_X = get_keras_data(train.loc[val_index], text_train[val_index])\n",
    "        dev_y, val_y = target[dev_index], target[val_index]\n",
    "        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test_X)\n",
    "        pred_val_y = pred_val_y.flatten()\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        cv_score = eval_fn(val_y, pred_val_y)\n",
    "        cv_scores.append(eval_fn(val_y, pred_val_y))\n",
    "        print_step(' cv score ' + str(i) + ' : ' + str(cv_score))\n",
    "        i += 1\n",
    "    print_step(' cv scores : ' + str(cv_scores))\n",
    "    print_step(' mean cv score : ' + str(np.mean(cv_scores)))\n",
    "    print_step(' std cv score : ' + str(np.std(cv_scores)))\n",
    "    pred_full_test = pred_full_test / 5.0\n",
    "    results = {'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-24 18:20:25.963430] Started  fold 1/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/4\n",
      " - 304s - loss: 0.0509 - mean_squared_error: 0.0509 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 2/4\n",
      " - 306s - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 3/4\n",
      " - 305s - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 4/4\n",
      " - 299s - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "[2018-06-24 18:40:43.303013] Loading best model\n",
      "[2018-06-24 18:40:43.835861] Predict Val 1/2\n",
      "[2018-06-24 18:41:39.369875] Predict Test 2/2\n",
      "[2018-06-24 18:43:12.453167]  cv score 1 : 0.21889769191455064\n",
      "[2018-06-24 18:43:12.462730] Started  fold 2/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/4\n",
      " - 303s - loss: 0.0510 - mean_squared_error: 0.0510 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 2/4\n",
      " - 301s - loss: 0.0479 - mean_squared_error: 0.0479 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 3/4\n",
      " - 307s - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 4/4\n",
      " - 299s - loss: 0.0444 - mean_squared_error: 0.0444 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "[2018-06-24 19:03:25.159722] Loading best model\n",
      "[2018-06-24 19:03:25.689623] Predict Val 1/2\n",
      "[2018-06-24 19:04:21.881759] Predict Test 2/2\n",
      "[2018-06-24 19:05:56.870604]  cv score 2 : 0.21855576804263846\n",
      "[2018-06-24 19:05:56.880365] Started  fold 3/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/4\n",
      " - 304s - loss: 0.0510 - mean_squared_error: 0.0510 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 2/4\n",
      " - 305s - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 3/4\n",
      " - 301s - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 4/4\n",
      " - 301s - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "[2018-06-24 19:26:09.882083] Loading best model\n",
      "[2018-06-24 19:26:10.411326] Predict Val 1/2\n",
      "[2018-06-24 19:27:07.923167] Predict Test 2/2\n",
      "[2018-06-24 19:28:45.344729]  cv score 3 : 0.2186759415297267\n",
      "[2018-06-24 19:28:45.354116] Started  fold 4/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/4\n",
      " - 309s - loss: 0.0510 - mean_squared_error: 0.0510 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 2/4\n",
      " - 301s - loss: 0.0479 - mean_squared_error: 0.0479 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 3/4\n",
      " - 306s - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 4/4\n",
      " - 304s - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "[2018-06-24 19:49:08.910672] Loading best model\n",
      "[2018-06-24 19:49:09.424960] Predict Val 1/2\n",
      "[2018-06-24 19:50:05.619114] Predict Test 2/2\n",
      "[2018-06-24 19:51:40.769283]  cv score 4 : 0.21847710161004666\n",
      "[2018-06-24 19:51:40.778609] Started  fold 5/5\n",
      "Train on 1202740 samples, validate on 300684 samples\n",
      "Epoch 1/4\n",
      " - 306s - loss: 0.0509 - mean_squared_error: 0.0509 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 2/4\n",
      " - 304s - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 3/4\n",
      " - 307s - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 4/4\n",
      " - 306s - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "[2018-06-24 20:12:05.721376] Loading best model\n",
      "[2018-06-24 20:12:06.258193] Predict Val 1/2\n",
      "[2018-06-24 20:13:03.887687] Predict Test 2/2\n",
      "[2018-06-24 20:14:41.430877]  cv score 5 : 0.21949340472318218\n",
      "[2018-06-24 20:14:41.430954]  cv scores : [0.21889769191455064, 0.21855576804263846, 0.2186759415297267, 0.21847710161004666, 0.21949340472318218]\n",
      "[2018-06-24 20:14:41.430989]  mean cv score : 0.2188199815640289\n",
      "[2018-06-24 20:14:41.431043]  std cv score : 0.0003654204780995126\n"
     ]
    }
   ],
   "source": [
    "results = run_cv_model(df_train, text_train, df_test, text_test, y, runCNN, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "# 0.2187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-24 21:14:01.428988] Skipped... Loaded cache/train_CNN_FastText.csv and cache/test_CNN_FastText.csv from cache!\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-24 21:14:01.686429] Skipped... Loaded cache/train_CNN_FastText_4.csv and cache/test_CNN_FastText_4.csv from cache!\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-24 21:14:01.943099] Skipped... Loaded cache/train_CNN_FastText_5.csv and cache/test_CNN_FastText_5.csv from cache!\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-24 21:14:02.199830] Skipped... Loaded cache/train_CNN_FastText_6.csv and cache/test_CNN_FastText_6.csv from cache!\n"
     ]
    }
   ],
   "source": [
    "train_cnn_1, test_cnn_1 = load_cache('CNN_FastText')\n",
    "train_cnn_4, test_cnn_4 = load_cache('CNN_FastText_4')\n",
    "train_cnn_5, test_cnn_5 = load_cache('CNN_FastText_5')\n",
    "train_cnn_6, test_cnn_6 = load_cache('CNN_FastText_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_1['cnn_4'] =train_cnn_4['CNN_FastText_4']\n",
    "train_cnn_1['cnn_5'] =train_cnn_5['CNN_FastText_5']\n",
    "train_cnn_1['cnn_6'] =train_cnn_6['CNN_FastText_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNN_FastText</th>\n",
       "      <th>cnn_4</th>\n",
       "      <th>cnn_5</th>\n",
       "      <th>cnn_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CNN_FastText</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929525</td>\n",
       "      <td>0.931626</td>\n",
       "      <td>0.928311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_4</th>\n",
       "      <td>0.929525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951353</td>\n",
       "      <td>0.946422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_5</th>\n",
       "      <td>0.931626</td>\n",
       "      <td>0.951353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_6</th>\n",
       "      <td>0.928311</td>\n",
       "      <td>0.946422</td>\n",
       "      <td>0.950345</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CNN_FastText     cnn_4     cnn_5     cnn_6\n",
       "CNN_FastText      1.000000  0.929525  0.931626  0.928311\n",
       "cnn_4             0.929525  1.000000  0.951353  0.946422\n",
       "cnn_5             0.931626  0.951353  1.000000  0.950345\n",
       "cnn_6             0.928311  0.946422  0.950345  1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cnn_1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[2018-06-24 21:05:01.795001] Prepping submission file\n",
      "[2018-06-24 21:05:03.739539] Done!\n"
     ]
    }
   ],
   "source": [
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print_step('Prepping submission file')\n",
    "submission = pd.read_csv(f'{data_dir}/test.csv', usecols=['item_id'])\n",
    "submission['deal_probability'] = results['test'].clip(0.0, 1.0)\n",
    "submission.to_csv('submit/submit_CNN_FastText_6.csv', index=False)\n",
    "print_step('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~\n",
      "[2018-06-24 21:13:39.073772] Cache\n",
      "[2018-06-24 21:13:41.950718] Saved cache/train_CNN_FastText_6.csv and cache/test_CNN_FastText_6.csv to cache!\n"
     ]
    }
   ],
   "source": [
    "print('~~~~~~~~~~')\n",
    "print_step('Cache')\n",
    "save_in_cache('CNN_FastText_6', pd.DataFrame({'CNN_FastText_6': results['train']}),\n",
    "                           pd.DataFrame({'CNN_FastText_6': results['test'].flatten()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

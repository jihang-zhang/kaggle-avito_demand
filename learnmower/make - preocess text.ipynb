{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# feature libraries\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn import preprocessing\n",
    "import os, re, regex, string, codecs\n",
    "import multiprocessing as mp\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/kaggle_data'\n",
    "text_dir = '../input/text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678[]{}- =_+() нийотстегивающийся «зубр более больше a а\n",
      "678 нийотстегивающийся зубр a\n",
      "нийотстегивающийся зубр a\n"
     ]
    }
   ],
   "source": [
    "# input/output ---------------\n",
    "def load_text(f):\n",
    "    text_cols = ['category_name', 'param_1', 'param_2', 'param_3', 'title', 'description']\n",
    "    df = pd.read_csv(f, usecols=text_cols, encoding='utf-8-sig').fillna('nan')\n",
    "    return df[text_cols].apply(lambda x: ''.join(x), axis=1).tolist()\n",
    "\n",
    "def save_text(file_name, text_list):\n",
    "    save_name = f'{text_dir}/{file_name}.txt'\n",
    "    file = codecs.open(save_name, 'w', 'utf-8-sig')\n",
    "    for i in text_list: file.write(f'{i}\\n')\n",
    "    file.close()\n",
    "    return \n",
    "\n",
    "# text processing ---------------\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('russian'))\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in text.split() if not w in STOPWORDS])\n",
    "\n",
    "    \n",
    "SPLIT_PATTERN = re.compile(u' |\\n')        \n",
    "def clean_text(text):\n",
    "    text = bytes(text, encoding='utf-8')\n",
    "    text = text.replace(b'\\n', b' ')\n",
    "    text = text.replace(b'\\t', b' ')\n",
    "    text = text.replace(b'\\b', b' ')\n",
    "    text = text.replace(b'\\r', b' ')\n",
    "    text = regex.sub(b'\\s+', b' ', text)\n",
    "    text = str(text, 'utf-8')\n",
    "    return ' '.join(re.split(SPLIT_PATTERN, text.strip()))\n",
    "\n",
    "# keep only alphanumeric and punctuation\n",
    "REGEX_0 = regex.compile(r'\\W+\\P+')\n",
    "def clean_level_0(text):\n",
    "    text = text.lower()\n",
    "    text = REGEX_0.sub(' ', text)\n",
    "    return clean_text(text)\n",
    "\n",
    "# remove punctuation and stopwords\n",
    "REGEX_1 = regex.compile(r'[\\W_]+')\n",
    "def clean_level_1(text):\n",
    "    text = text.lower()\n",
    "    text = REGEX_1.sub(' ', text)\n",
    "    text = remove_stopwords(text)\n",
    "    return clean_text(text)    \n",
    "\n",
    "# keep only letters\n",
    "REGEX_2 = regex.compile(r'\\d+')\n",
    "def clean_level_2(text):\n",
    "    text = text.lower()\n",
    "    text = REGEX_1.sub(' ', text)\n",
    "    text = REGEX_2.sub(' ', text)\n",
    "    text = remove_stopwords(text)\n",
    "    return clean_text(text)    \n",
    "\n",
    "# tokenize words\n",
    "def tokenize(text_list):\n",
    "    pattern = re.compile(u' |\\n')\n",
    "    text = {w for text in text_list for w in re.split(pattern, text.strip())}\n",
    "    \n",
    "    word_tokenizer = Tokenizer(filters='', lower=False)\n",
    "    word_tokenizer.fit_on_texts(text)\n",
    "    return list(word_tokenizer.word_index.keys())\n",
    "\n",
    "print(clean_level_0('\\n678[]{}-\\r=_+() \\nнийотстегивающийся «зубр более больше   a а'))\n",
    "print(clean_level_1('\\n678[]{}-\\r=_+() \\nнийотстегивающийся «зубр более больше   a а'))\n",
    "print(clean_level_2('\\n678[]{}-\\r=_+() \\nнийотстегивающийся «зубр более больше   a а'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "test = load_text(f'{data_dir}/test.csv')\n",
    "train = load_text(f'{data_dir}/train.csv')\n",
    "test_active = load_text(f'{data_dir}/test_active.csv')\n",
    "train_active = load_text(f'{data_dir}/train_active.csv')\n",
    "\n",
    "# setup multiprocessing\n",
    "run_parallel = False\n",
    "if run_parallel: \n",
    "    p = mp.Pool(12)\n",
    "\n",
    "cleaners = [clean_level_0, clean_level_1, clean_level_2]\n",
    "for i in range(3):\n",
    "        \n",
    "    print(i)\n",
    "    \n",
    "    # clean text\n",
    "    if run_parallel:      \n",
    "        test_cleaned = p.map(cleaners[i], test)\n",
    "        train_cleaned = p.map(cleaners[i], train)\n",
    "        test_active_cleaned = p.map(cleaners[i], test_active)\n",
    "        train_active_cleaned = p.map(cleaners[i], train_active)\n",
    "    else:\n",
    "        cleaner = cleaners[i]\n",
    "        test_cleaned = [cleaner(x) for x in test]\n",
    "        train_cleaned = [cleaner(x) for x in train]\n",
    "        test_active_cleaned = [cleaner(x) for x in test_active]\n",
    "        train_active_cleaned = [cleaner(x) for x in train_active]\n",
    "\n",
    "    # combine\n",
    "    full_cleaned = train_cleaned+test_cleaned+train_active_cleaned+test_active_cleaned\n",
    "    save_text(f'text_{i}', full_cleaned)\n",
    "    save_text(f'test_text_{i}', test_cleaned)\n",
    "    save_text(f'train_text_{i}', train_cleaned)\n",
    "    del full_cleaned, test_active_cleaned, train_active_cleaned\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = ['token']+tokenize(test_cleaned+train_cleaned)\n",
    "    save_text(f'tokens_{i}', tokens)\n",
    "    del test_cleaned, train_cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

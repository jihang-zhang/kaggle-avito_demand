{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijunhe/Environment/ML_py_36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from cv import run_cv_model\n",
    "from utils import print_step, rmse\n",
    "from cache import get_data, is_in_cache, load_cache, save_in_cache\n",
    "\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, concatenate, BatchNormalization, Flatten, Concatenate, Conv1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[2018-06-26 22:19:57.361160] Importing Data 1/15\n",
      "Train shape: (1503424, 18)\n",
      "Test shape: (508438, 17)\n",
      "~~~~~~~~~~~~~~~\n",
      "[2018-06-26 22:20:08.658135] Subsetting\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[2018-06-26 22:20:09.023897] Importing Data 2/15\n",
      "Test shape: (508438, 61)\n",
      "Train shape: (1503424, 61)\n",
      "[2018-06-26 22:20:19.866264] Skipped... Loaded cache/train_data_with_fe.csv and cache/test_data_with_fe.csv from cache!\n",
      "[2018-06-26 22:20:19.891184] Importing Data 2/15\n",
      "Test shape: (508438, 63)\n",
      "Train shape: (1503424, 63)\n",
      "[2018-06-26 22:20:40.138892] Skipped... Loaded cache/train_tfidf_ridges.csv and cache/test_tfidf_ridges.csv from cache!\n",
      "[2018-06-26 22:20:40.164816] Importing Data 2/15\n",
      "[2018-06-26 22:20:40.170143] Importing Data 3/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:20:40.456519] Skipped... Loaded cache/train_base_lgb2.csv and cache/test_base_lgb2.csv from cache!\n",
      "[2018-06-26 22:20:40.456656] Importing Data 3/15 2/3\n",
      "[2018-06-26 22:20:40.457882] Importing Data 3/15 3/3\n",
      "[2018-06-26 22:20:40.458386] Importing Data 3/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:20:40.745610] Skipped... Loaded cache/train_te_lgb3.csv and cache/test_te_lgb3.csv from cache!\n",
      "[2018-06-26 22:20:40.745758] Importing Data 3/15 2/3\n",
      "[2018-06-26 22:20:40.747055] Importing Data 3/15 3/3\n",
      "[2018-06-26 22:20:40.747630] Importing Data 3/15 1/3\n",
      "Test shape: (508438, 2)\n",
      "Train shape: (1503424, 2)\n",
      "[2018-06-26 22:20:41.558875] Skipped... Loaded cache/train_ryan_lgbm_v29.csv and cache/test_ryan_lgbm_v29.csv from cache!\n",
      "[2018-06-26 22:20:41.574323] Importing Data 3/15 2/3\n",
      "[2018-06-26 22:20:41.575623] Importing Data 3/15 3/3\n",
      "[2018-06-26 22:20:41.576182] Importing Data 3/15 1/3\n",
      "Test shape: (508438, 2)\n",
      "Train shape: (1503424, 2)\n",
      "[2018-06-26 22:20:42.380076] Skipped... Loaded cache/train_ryan_lgbm_v33.csv and cache/test_ryan_lgbm_v33.csv from cache!\n",
      "[2018-06-26 22:20:42.395017] Importing Data 3/15 2/3\n",
      "[2018-06-26 22:20:42.396298] Importing Data 3/15 3/3\n",
      "[2018-06-26 22:20:42.396943] Importing Data 3/15 1/3\n",
      "Test shape: (508438, 2)\n",
      "Train shape: (1503424, 2)\n",
      "[2018-06-26 22:20:43.215245] Skipped... Loaded cache/train_ryan_lgbm_v36.csv and cache/test_ryan_lgbm_v36.csv from cache!\n",
      "[2018-06-26 22:20:43.230694] Importing Data 3/15 2/3\n",
      "[2018-06-26 22:20:43.232001] Importing Data 3/15 3/3\n",
      "[2018-06-26 22:20:43.232638] Importing Data 3/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:20:43.516078] Skipped... Loaded cache/train_ridge_lgb3.csv and cache/test_ridge_lgb3.csv from cache!\n",
      "[2018-06-26 22:20:43.516310] Importing Data 3/15 2/3\n",
      "[2018-06-26 22:20:43.517569] Importing Data 3/15 3/3\n",
      "[2018-06-26 22:20:43.518159] Importing Data 3/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:20:43.800748] Skipped... Loaded cache/train_ridge_lgb_poisson.csv and cache/test_ridge_lgb_poisson.csv from cache!\n",
      "[2018-06-26 22:20:43.800824] Importing Data 3/15 2/3\n",
      "[2018-06-26 22:20:43.802074] Importing Data 3/15 3/3\n",
      "[2018-06-26 22:20:43.802650] Importing Data 4/15 1/4\n",
      "Test shape: (508438, 21)\n",
      "Train shape: (1503424, 22)\n",
      "[2018-06-26 22:20:57.135825] Skipped... Loaded cache/train_parent_cat_ridges.csv and cache/test_parent_cat_ridges.csv from cache!\n",
      "[2018-06-26 22:20:57.135917] Importing Data 4/15 2/4\n",
      "[2018-06-26 22:20:57.143974] Importing Data 4/15 3/4\n",
      "[2018-06-26 22:20:57.184196] Importing Data 4/15 4/4\n",
      "[2018-06-26 22:20:57.216385] Importing Data 5/15 1/4\n",
      "Test shape: (508438, 22)\n",
      "Train shape: (1503424, 23)\n",
      "[2018-06-26 22:21:11.513287] Skipped... Loaded cache/train_parent_regioncat_ridges.csv and cache/test_parent_regioncat_ridges.csv from cache!\n",
      "[2018-06-26 22:21:11.513374] Importing Data 5/15 2/4\n",
      "[2018-06-26 22:21:11.521263] Importing Data 5/15 3/4\n",
      "[2018-06-26 22:21:11.679035] Importing Data 5/15 4/4\n",
      "[2018-06-26 22:21:11.698987] Importing Data 6/15 1/4\n",
      "Test shape: (508438, 23)\n",
      "Train shape: (1503424, 23)\n",
      "[2018-06-26 22:21:26.463078] Skipped... Loaded cache/train_cat_bin_ridges.csv and cache/test_cat_bin_ridges.csv from cache!\n",
      "[2018-06-26 22:21:26.464125] Importing Data 6/15 2/4\n",
      "[2018-06-26 22:21:26.472322] Importing Data 6/15 3/4\n",
      "[2018-06-26 22:21:26.674052] Importing Data 6/15 4/4\n",
      "[2018-06-26 22:21:26.714048] Importing Data 7/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:26.997097] Skipped... Loaded cache/train_deep_lgb.csv and cache/test_deep_lgb.csv from cache!\n",
      "[2018-06-26 22:21:26.997220] Importing Data 7/15 2/3\n",
      "[2018-06-26 22:21:26.998535] Importing Data 7/15 3/3\n",
      "[2018-06-26 22:21:26.999190] Importing Data 7/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:27.283812] Skipped... Loaded cache/train_deep_lgb2.csv and cache/test_deep_lgb2.csv from cache!\n",
      "[2018-06-26 22:21:27.283895] Importing Data 7/15 2/3\n",
      "[2018-06-26 22:21:27.285125] Importing Data 7/15 3/3\n",
      "[2018-06-26 22:21:27.285702] Importing Data 7/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:27.570877] Skipped... Loaded cache/train_deep_lgb3.csv and cache/test_deep_lgb3.csv from cache!\n",
      "[2018-06-26 22:21:27.570961] Importing Data 7/15 2/3\n",
      "[2018-06-26 22:21:27.572205] Importing Data 7/15 3/3\n",
      "[2018-06-26 22:21:27.572915] Importing Data 8/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:27.858392] Skipped... Loaded cache/train_full_text_ridge.csv and cache/test_full_text_ridge.csv from cache!\n",
      "[2018-06-26 22:21:27.858475] Importing Data 8/15 2/3\n",
      "[2018-06-26 22:21:27.859767] Importing Data 8/15 3/3\n",
      "[2018-06-26 22:21:27.860309] Importing Data 9/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:28.147038] Skipped... Loaded cache/train_complete_ridge.csv and cache/test_complete_ridge.csv from cache!\n",
      "[2018-06-26 22:21:28.147256] Importing Data 9/15 2/3\n",
      "[2018-06-26 22:21:28.148466] Importing Data 9/15 3/3\n",
      "[2018-06-26 22:21:28.149038] Importing Data 9/15 1/3\n",
      "Test shape: (508438, 3)\n",
      "Train shape: (1503424, 3)\n",
      "[2018-06-26 22:21:29.234359] Skipped... Loaded cache/train_ryan_ridge_sgd_v2.csv and cache/test_ryan_ridge_sgd_v2.csv from cache!\n",
      "[2018-06-26 22:21:29.250287] Importing Data 9/15 2/3\n",
      "[2018-06-26 22:21:29.251624] Importing Data 9/15 3/3\n",
      "[2018-06-26 22:21:29.252249] Importing Data 9/15 2/3\n",
      "[2018-06-26 22:21:29.253409] Importing Data 9/15 3/3\n",
      "[2018-06-26 22:21:29.254033] Importing Data 10/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:29.538106] Skipped... Loaded cache/train_complete_fm.csv and cache/test_complete_fm.csv from cache!\n",
      "[2018-06-26 22:21:29.538214] Importing Data 10/15 2/3\n",
      "[2018-06-26 22:21:29.539522] Importing Data 10/15 3/3\n",
      "[2018-06-26 22:21:29.540128] Importing Data 11/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:29.798466] Skipped... Loaded cache/train_tffm2.csv and cache/test_tffm2.csv from cache!\n",
      "[2018-06-26 22:21:29.798665] Importing Data 11/15 2/3\n",
      "[2018-06-26 22:21:29.799872] Importing Data 11/15 3/3\n",
      "[2018-06-26 22:21:29.800375] Importing Data 12/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:30.056166] Skipped... Loaded cache/train_tffm3.csv and cache/test_tffm3.csv from cache!\n",
      "[2018-06-26 22:21:30.056378] Importing Data 12/15 2/3\n",
      "[2018-06-26 22:21:30.057714] Importing Data 12/15 3/3\n",
      "[2018-06-26 22:21:30.058263] Importing Data 13/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:30.312213] Skipped... Loaded cache/train_CNN_FastText.csv and cache/test_CNN_FastText.csv from cache!\n",
      "[2018-06-26 22:21:30.312291] Importing Data 13/15 2/3\n",
      "[2018-06-26 22:21:30.313523] Importing Data 13/15 3/3\n",
      "[2018-06-26 22:21:30.314029] Importing Data 14/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:30.569948] Skipped... Loaded cache/train_CNN_FastText_4.csv and cache/test_CNN_FastText_4.csv from cache!\n",
      "[2018-06-26 22:21:30.570027] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:30.571237] Importing Data 14/15 3/3\n",
      "[2018-06-26 22:21:30.571808] Importing Data 14/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:30.828038] Skipped... Loaded cache/train_RNN_AttentionPooling.csv and cache/test_RNN_AttentionPooling.csv from cache!\n",
      "[2018-06-26 22:21:30.828118] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:30.829368] Importing Data 14/15 3/3\n",
      "[2018-06-26 22:21:30.829931] Importing Data 14/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:31.087609] Skipped... Loaded cache/train_RNN_AttentionPooling_img2.csv and cache/test_RNN_AttentionPooling_img2.csv from cache!\n",
      "[2018-06-26 22:21:31.087687] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:31.088936] Importing Data 14/15 3/3\n",
      "[2018-06-26 22:21:31.089588] Importing Data 14/15 1/3\n",
      "[2018-06-26 22:21:31.938244] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:31.939715] Importing Data 14/15 3/3\n",
      "[2018-06-26 22:21:31.940436] Importing Data 14/15 1/3\n",
      "[2018-06-26 22:21:33.350911] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:34.349649] Importing Data 14/15 3/3\n",
      "[2018-06-26 22:21:34.656155] Importing Data 14/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:34.901428] Skipped... Loaded cache/train_CNN_binary_PL.csv and cache/test_CNN_binary_PL.csv from cache!\n",
      "[2018-06-26 22:21:34.917104] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:34.918421] Importing Data 14/15 3/3\n",
      "[2018-06-26 22:21:34.919031] Importing Data 14/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:35.141641] Skipped... Loaded cache/train_liu_nn.csv and cache/test_liu_nn.csv from cache!\n",
      "[2018-06-26 22:21:35.141859] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:35.143200] Importing Data 14/15 3/3\n",
      "[2018-06-26 22:21:35.143776] Importing Data 14/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:35.367164] Skipped... Loaded cache/train_liu_nn2.csv and cache/test_liu_nn2.csv from cache!\n",
      "[2018-06-26 22:21:35.367271] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:35.368520] Importing Data 14/15 3/3\n",
      "[2018-06-26 22:21:35.369079] Importing Data 14/15 1/3\n",
      "Test shape: (508438, 1)\n",
      "Train shape: (1503424, 1)\n",
      "[2018-06-26 22:21:35.646829] Skipped... Loaded cache/train_liu_lgb.csv and cache/test_liu_lgb.csv from cache!\n",
      "[2018-06-26 22:21:35.646986] Importing Data 14/15 2/3\n",
      "[2018-06-26 22:21:35.648222] Importing Data 14/15 3/3\n"
     ]
    }
   ],
   "source": [
    "print('~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print_step('Importing Data 1/15')\n",
    "train, test = get_data()\n",
    "\n",
    "print('~~~~~~~~~~~~~~~')\n",
    "print_step('Subsetting')\n",
    "target = train['deal_probability']\n",
    "train_id = train['item_id']\n",
    "test_id = test['item_id']\n",
    "train.drop(['deal_probability', 'item_id'], axis=1, inplace=True)\n",
    "test.drop(['item_id'], axis=1, inplace=True)\n",
    "\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print_step('Importing Data 2/15')\n",
    "train_fe, test_fe = load_cache('data_with_fe')\n",
    "\n",
    "print_step('Importing Data 2/15')\n",
    "train_ridge, test_ridge = load_cache('tfidf_ridges')\n",
    "drops = [c for c in train_ridge.columns if 'svd' in c or 'tfidf' in c]\n",
    "train_ridge.drop(drops, axis=1, inplace=True)\n",
    "test_ridge.drop(drops, axis=1, inplace=True)\n",
    "train_ = train_ridge\n",
    "test_ = test_ridge\n",
    "\n",
    "print_step('Importing Data 2/15')\n",
    "train_['parent_category_name'] = train_fe['parent_category_name']\n",
    "test_['parent_category_name'] = test_fe['parent_category_name']\n",
    "train_['price'] = train_fe['price']\n",
    "test_['price'] = test_fe['price']\n",
    "\n",
    "# print_step('Importing Data 3/15 1/3')\n",
    "# train_base_lgb, test_base_lgb = load_cache('base_lgb')\n",
    "# print_step('Importing Data 3/15 2/3')\n",
    "# train_['base_lgb'] = train_base_lgb['base_lgb']\n",
    "# print_step('Importing Data 3/15 3/3')\n",
    "# test_['base_lgb'] = test_base_lgb['base_lgb']\n",
    "\n",
    "print_step('Importing Data 3/15 1/3')\n",
    "train_base_lgb, test_base_lgb = load_cache('base_lgb2')\n",
    "print_step('Importing Data 3/15 2/3')\n",
    "train_['base_lgb2'] = train_base_lgb['base_lgb2']\n",
    "print_step('Importing Data 3/15 3/3')\n",
    "test_['base_lgb2'] = test_base_lgb['base_lgb2']\n",
    "\n",
    "# train_te_lgb, test_te_lgb = load_cache('te_lgb')\n",
    "# print_step('Importing Data 3/15 2/3')\n",
    "# train_['te_lgb'] = train_te_lgb['te_lgb']\n",
    "# print_step('Importing Data 3/15 3/3')\n",
    "# test_['te_lgb'] = test_te_lgb['te_lgb']\n",
    "\n",
    "# print_step('Importing Data 3/15 1/3')\n",
    "# train_te_lgb2, test_te_lgb2 = load_cache('te_lgb2')\n",
    "# print_step('Importing Data 3/15 2/3')\n",
    "# train_['te_lgb2'] = train_te_lgb2['te_lgb']\n",
    "# print_step('Importing Data 3/15 3/3')\n",
    "# test_['te_lgb2'] = test_te_lgb2['te_lgb']\n",
    "\n",
    "print_step('Importing Data 3/15 1/3')\n",
    "train_te_lgb2, test_te_lgb2 = load_cache('te_lgb3')\n",
    "print_step('Importing Data 3/15 2/3')\n",
    "train_['te_lgb3'] = train_te_lgb2['te_lgb3']\n",
    "print_step('Importing Data 3/15 3/3')\n",
    "test_['te_lgb3'] = test_te_lgb2['te_lgb3']\n",
    "\n",
    "# print_step('Importing Data 3/15 1/3')\n",
    "# train_te_lgb2, test_te_lgb2 = load_cache('te_lgb_poisson')\n",
    "# print_step('Importing Data 3/15 2/3')\n",
    "# train_['te_lgb_poisson'] = train_te_lgb2['te_lgb_poisson']\n",
    "# print_step('Importing Data 3/15 3/3')\n",
    "# test_['te_lgb_poisson'] = test_te_lgb2['te_lgb_poisson']\n",
    "\n",
    "print_step('Importing Data 3/15 1/3')\n",
    "train_ryan_lgbm_v29, test_ryan_lgbm_v29 = load_cache('ryan_lgbm_v29')\n",
    "print_step('Importing Data 3/15 2/3')\n",
    "train_['ryan_lgbm_v29'] = train_ryan_lgbm_v29['oof_lgbm']\n",
    "print_step('Importing Data 3/15 3/3')\n",
    "test_['ryan_lgbm_v29'] = test_ryan_lgbm_v29['oof_lgbm']\n",
    "\n",
    "print_step('Importing Data 3/15 1/3')\n",
    "train_ryan_lgbm_v29, test_ryan_lgbm_v29 = load_cache('ryan_lgbm_v33')\n",
    "print_step('Importing Data 3/15 2/3')\n",
    "train_['ryan_lgbm_v33'] = train_ryan_lgbm_v29['oof_lgbm']\n",
    "print_step('Importing Data 3/15 3/3')\n",
    "test_['ryan_lgbm_v33'] = test_ryan_lgbm_v29['oof_lgbm']\n",
    "\n",
    "print_step('Importing Data 3/15 1/3')\n",
    "train_ryan_lgbm_v29, test_ryan_lgbm_v29 = load_cache('ryan_lgbm_v36')\n",
    "print_step('Importing Data 3/15 2/3')\n",
    "train_['ryan_lgbm_v36'] = train_ryan_lgbm_v29['oof_lgbm']\n",
    "print_step('Importing Data 3/15 3/3')\n",
    "test_['ryan_lgbm_v36'] = test_ryan_lgbm_v29['oof_lgbm']\n",
    "\n",
    "# print_step('Importing Data 3/15 1/3')\n",
    "# train_ridge_lgb, test_ridge_lgb = load_cache('ridge_lgb')\n",
    "# print_step('Importing Data 3/15 2/3')\n",
    "# train_['ridge_lgb'] = train_ridge_lgb['ridge_lgb']\n",
    "# print_step('Importing Data 3/15 3/3')\n",
    "# test_['ridge_lgb'] = test_ridge_lgb['ridge_lgb']\n",
    "\n",
    "# print_step('Importing Data 3/15 1/3')\n",
    "# train_ridge_lgb, test_ridge_lgb = load_cache('ridge_lgb2')\n",
    "# print_step('Importing Data 3/15 2/3')\n",
    "# train_['ridge_lgb2'] = train_ridge_lgb['ridge_lgb2']\n",
    "# print_step('Importing Data 3/15 3/3')\n",
    "# test_['ridge_lgb2'] = test_ridge_lgb['ridge_lgb2']\n",
    "\n",
    "print_step('Importing Data 3/15 1/3')\n",
    "train_ridge_lgb, test_ridge_lgb = load_cache('ridge_lgb3')\n",
    "print_step('Importing Data 3/15 2/3')\n",
    "train_['ridge_lgb3'] = train_ridge_lgb['ridge_lgb3']\n",
    "print_step('Importing Data 3/15 3/3')\n",
    "test_['ridge_lgb3'] = test_ridge_lgb['ridge_lgb3']\n",
    "\n",
    "print_step('Importing Data 3/15 1/3')\n",
    "train_ridge_lgb, test_ridge_lgb = load_cache('ridge_lgb_poisson')\n",
    "print_step('Importing Data 3/15 2/3')\n",
    "train_['ridge_lgb_poisson'] = train_ridge_lgb['ridge_lgb_poisson']\n",
    "print_step('Importing Data 3/15 3/3')\n",
    "test_['ridge_lgb_poisson'] = test_ridge_lgb['ridge_lgb_poisson']\n",
    "\n",
    "print_step('Importing Data 4/15 1/4')\n",
    "train_pcat_ridge, test_pcat_ridge = load_cache('parent_cat_ridges')\n",
    "print_step('Importing Data 4/15 2/4')\n",
    "train_pcat_ridge = train_pcat_ridge[[c for c in train_pcat_ridge.columns if 'ridge' in c]]\n",
    "test_pcat_ridge = test_pcat_ridge[[c for c in test_pcat_ridge.columns if 'ridge' in c]]\n",
    "print_step('Importing Data 4/15 3/4')\n",
    "train_ = pd.concat([train_, train_pcat_ridge], axis=1)\n",
    "print_step('Importing Data 4/15 4/4')\n",
    "test_ = pd.concat([test_, test_pcat_ridge], axis=1)\n",
    "\n",
    "print_step('Importing Data 5/15 1/4')\n",
    "train_rcat_ridge, test_rcat_ridge = load_cache('parent_regioncat_ridges')\n",
    "print_step('Importing Data 5/15 2/4')\n",
    "train_rcat_ridge = train_rcat_ridge[[c for c in train_rcat_ridge.columns if 'ridge' in c]]\n",
    "test_rcat_ridge = test_rcat_ridge[[c for c in test_rcat_ridge.columns if 'ridge' in c]]\n",
    "print_step('Importing Data 5/15 3/4')\n",
    "train_ = pd.concat([train_, train_rcat_ridge], axis=1)\n",
    "print_step('Importing Data 5/15 4/4')\n",
    "test_ = pd.concat([test_, test_rcat_ridge], axis=1)\n",
    "\n",
    "print_step('Importing Data 6/15 1/4')\n",
    "train_catb_ridge, test_catb_ridge = load_cache('cat_bin_ridges')\n",
    "print_step('Importing Data 6/15 2/4')\n",
    "train_catb_ridge = train_catb_ridge[[c for c in train_catb_ridge.columns if 'ridge' in c]]\n",
    "test_catb_ridge = test_catb_ridge[[c for c in test_catb_ridge.columns if 'ridge' in c]]\n",
    "print_step('Importing Data 6/15 3/4')\n",
    "train_ = pd.concat([train_, train_catb_ridge], axis=1)\n",
    "print_step('Importing Data 6/15 4/4')\n",
    "test_ = pd.concat([test_, test_catb_ridge], axis=1)\n",
    "\n",
    "print_step('Importing Data 7/15 1/3')\n",
    "train_deep_lgb, test_deep_lgb = load_cache('deep_lgb')\n",
    "print_step('Importing Data 7/15 2/3')\n",
    "train_['deep_lgb'] = train_deep_lgb['deep_lgb']\n",
    "print_step('Importing Data 7/15 3/3')\n",
    "test_['deep_lgb'] = test_deep_lgb['deep_lgb']\n",
    "\n",
    "print_step('Importing Data 7/15 1/3')\n",
    "train_deep_lgb, test_deep_lgb = load_cache('deep_lgb2')\n",
    "print_step('Importing Data 7/15 2/3')\n",
    "train_['deep_lgb2'] = train_deep_lgb['deep_lgb2']\n",
    "print_step('Importing Data 7/15 3/3')\n",
    "test_['deep_lgb2'] = test_deep_lgb['deep_lgb2']\n",
    "\n",
    "print_step('Importing Data 7/15 1/3')\n",
    "train_deep_lgb, test_deep_lgb = load_cache('deep_lgb3')\n",
    "print_step('Importing Data 7/15 2/3')\n",
    "train_['deep_lgb3'] = train_deep_lgb['deep_lgb3']\n",
    "print_step('Importing Data 7/15 3/3')\n",
    "test_['deep_lgb3'] = test_deep_lgb['deep_lgb3']\n",
    "\n",
    "print_step('Importing Data 8/15 1/3')\n",
    "train_full_text_ridge, test_full_text_ridge = load_cache('full_text_ridge')\n",
    "print_step('Importing Data 8/15 2/3')\n",
    "train_['full_text_ridge'] = train_full_text_ridge['full_text_ridge']\n",
    "print_step('Importing Data 8/15 3/3')\n",
    "test_['full_text_ridge'] = test_full_text_ridge['full_text_ridge']\n",
    "\n",
    "print_step('Importing Data 9/15 1/3')\n",
    "train_complete_ridge, test_complete_ridge = load_cache('complete_ridge')\n",
    "print_step('Importing Data 9/15 2/3')\n",
    "train_['complete_ridge'] = train_complete_ridge['complete_ridge']\n",
    "print_step('Importing Data 9/15 3/3')\n",
    "test_['complete_ridge'] = test_complete_ridge['complete_ridge']\n",
    "\n",
    "print_step('Importing Data 9/15 1/3')\n",
    "train_ryan_ridge, test_ryan_ridge = load_cache('ryan_ridge_sgd_v2')\n",
    "print_step('Importing Data 9/15 2/3')\n",
    "train_['ryan_ridge'] = train_ryan_ridge['oof_ridge']\n",
    "print_step('Importing Data 9/15 3/3')\n",
    "test_['ryan_ridge'] = test_ryan_ridge['oof_ridge']\n",
    "print_step('Importing Data 9/15 2/3')\n",
    "train_['ryan_sgd'] = train_ryan_ridge['oof_sgd']\n",
    "print_step('Importing Data 9/15 3/3')\n",
    "test_['ryan_sgd'] = test_ryan_ridge['oof_sgd']\n",
    "\n",
    "print_step('Importing Data 10/15 1/3')\n",
    "train_complete_fm, test_complete_fm = load_cache('complete_fm')\n",
    "print_step('Importing Data 10/15 2/3')\n",
    "train_['complete_fm'] = train_complete_fm['complete_fm']\n",
    "print_step('Importing Data 10/15 3/3')\n",
    "test_['complete_fm'] = test_complete_fm['complete_fm']\n",
    "\n",
    "print_step('Importing Data 11/15 1/3')\n",
    "train_tffm2, test_tffm2 = load_cache('tffm2')\n",
    "print_step('Importing Data 11/15 2/3')\n",
    "train_['tffm2'] = train_tffm2['tffm2']\n",
    "print_step('Importing Data 11/15 3/3')\n",
    "test_['tffm2'] = test_tffm2['tffm2']\n",
    "\n",
    "print_step('Importing Data 12/15 1/3')\n",
    "train_tffm3, test_tffm3 = load_cache('tffm3')\n",
    "print_step('Importing Data 12/15 2/3')\n",
    "train_['tffm3'] = train_tffm3['tffm3']\n",
    "print_step('Importing Data 12/15 3/3')\n",
    "test_['tffm3'] = test_tffm3['tffm3']\n",
    "\n",
    "print_step('Importing Data 13/15 1/3')\n",
    "train_cnn_ft, test_cnn_ft = load_cache('CNN_FastText')\n",
    "print_step('Importing Data 13/15 2/3')\n",
    "train_['cnn_ft'] = train_cnn_ft['CNN_FastText']\n",
    "print_step('Importing Data 13/15 3/3')\n",
    "test_['cnn_ft'] = test_cnn_ft['CNN_FastText']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_cnn_ft, test_cnn_ft = load_cache('CNN_FastText_4')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_['cnn_ft4'] = train_cnn_ft['CNN_FastText_4']\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_['cnn_ft4'] = test_cnn_ft['CNN_FastText_4']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_cnn_ft, test_cnn_ft = load_cache('RNN_AttentionPooling')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_['rnn_at'] = train_cnn_ft['RNN_AttentionPooling']\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_['rnn_at'] = test_cnn_ft['RNN_AttentionPooling']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_cnn_ft, test_cnn_ft = load_cache('RNN_AttentionPooling_img2')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_['rnn_at2'] = train_cnn_ft['RNN_AttentionPooling_img2']\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_['rnn_at2'] = test_cnn_ft['RNN_AttentionPooling_img2']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_cnn_ft = pd.read_csv('cache/matt_nn_oof.csv')\n",
    "test_cnn_ft = pd.read_csv('cache/matt_nn_test.csv')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_['matt_nn'] = train_cnn_ft['matt_nn']\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_['matt_nn'] = test_cnn_ft['deal_probability']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_multi = pd.read_csv('cache/matt_multi_nn_oof.csv')\n",
    "test_multi = pd.read_csv('cache/matt_multi_nn_test.csv')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_ = pd.concat([train_, train_multi], axis=1)\n",
    "train_.drop('item_id', axis=1, inplace=True)\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_ = pd.concat([test_, test_multi], axis=1)\n",
    "test_.drop('item_id', axis=1, inplace=True)\n",
    "\n",
    "# print_step('Importing Data 14/15 1/3')\n",
    "# train_cnn_ft, test_cnn_ft = load_cache('CNN_binary')\n",
    "# print_step('Importing Data 14/15 2/3')\n",
    "# train_['CNN_binary'] = train_cnn_ft['CNN_binary']\n",
    "# print_step('Importing Data 14/15 3/3')\n",
    "# test_['CNN_binary'] = test_cnn_ft['CNN_binary']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_cnn_ft, test_cnn_ft = load_cache('CNN_binary_PL')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_['CNN_binary_PL'] = train_cnn_ft['CNN_binary_PL']\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_['CNN_binary_PL'] = test_cnn_ft['CNN_binary_PL']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_liu_nn, test_liu_nn = load_cache('liu_nn')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_['liu_nn'] = train_liu_nn['liu_nn']\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_['liu_nn'] = test_liu_nn['liu_nn']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_liu_nn, test_liu_nn = load_cache('liu_nn2')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_['liu_nn2'] = train_liu_nn['liu_nn2']\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_['liu_nn2'] = test_liu_nn['liu_nn2']\n",
    "\n",
    "print_step('Importing Data 14/15 1/3')\n",
    "train_liu_lgb, test_liu_lgb = load_cache('liu_lgb')\n",
    "print_step('Importing Data 14/15 2/3')\n",
    "train_['liu_lgb'] = train_liu_lgb['liu_lgb']\n",
    "print_step('Importing Data 14/15 3/3')\n",
    "test_['liu_lgb'] = test_liu_lgb['liu_lgb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_,test_],axis=0)\n",
    "num_train = len(train)\n",
    "df['price'] = np.log1p(df['price'])\n",
    "num_cols = [i for i in df.columns if i != 'parent_category_name']\n",
    "for c in num_cols:\n",
    "    df[c] = (df[c] - np.mean(df[c]))/np.std(df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "cat_cols = ['parent_category_name']\n",
    "for c in cat_cols:\n",
    "    df[c] = lbl.fit_transform(df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = df[:num_train]\n",
    "test_ = df[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_category = max(df['parent_category_name']) + 1\n",
    "num_cols = [i for i in df.columns if i != 'parent_category_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(): \n",
    "        \n",
    "    K.clear_session()\n",
    "\n",
    "    parent_cat = Input(shape=[1], name='parent_category_name')\n",
    "    nums = [Input(shape=[1], name=name) for name in num_cols]\n",
    "    embeded = Flatten() (Embedding(max_category, 50) (parent_cat))\n",
    "    x = concatenate(nums + [embeded])\n",
    "    for num_dense in [256, 64, 16]:\n",
    "        x = Dense(num_dense, activation='relu') (x)\n",
    "#         x = Dropout(0.1)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=[parent_cat]+nums, outputs=output)\n",
    "    model.compile(loss = 'mse',\n",
    "                  metrics=['mse'],\n",
    "                  optimizer='nadam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(df):\n",
    "    X = {}\n",
    "    for c in df.columns:\n",
    "        X[c] = df[c].values\n",
    "    return X\n",
    "def run_cv_model(train, test, target, model_fn, eval_fn):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    fold_splits = kf.split(train)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros(train.shape[0])\n",
    "    i = 1\n",
    "    test_X = get_keras_data(test)\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print_step('Started ' + ' fold ' + str(i) + '/5')\n",
    "        dev_X = get_keras_data(train.loc[dev_index])\n",
    "        val_X = get_keras_data(train.loc[val_index])\n",
    "        dev_y, val_y = target[dev_index], target[val_index]\n",
    "        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test_X)\n",
    "        pred_val_y = pred_val_y.flatten()\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        cv_score = eval_fn(val_y, pred_val_y)\n",
    "        cv_scores.append(eval_fn(val_y, pred_val_y))\n",
    "        print_step(' cv score ' + str(i) + ' : ' + str(cv_score))\n",
    "        i += 1\n",
    "    print_step(' cv scores : ' + str(cv_scores))\n",
    "    print_step(' mean cv score : ' + str(np.mean(cv_scores)))\n",
    "    print_step(' std cv score : ' + str(np.std(cv_scores)))\n",
    "    pred_full_test = pred_full_test / 5.0\n",
    "    results = {'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMLP(train_X, train_y, val_X, val_y, test_X):\n",
    "    model = MLP()\n",
    "    \n",
    "    ### Callbacks\n",
    "    def schedule(ind):\n",
    "        return(1e-3/(1.5**(ind)))\n",
    "\n",
    "    lr = LearningRateScheduler(schedule)\n",
    "    \n",
    "#     tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "#                           write_graph=True, write_images=False)\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(filepath = 'tmp_bst_mdl.hdf5', monitor='val_mean_squared_error', verbose=0, save_best_only = True, mode='min')\n",
    "\n",
    "#     tb = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    model.fit(train_X, train_y, \n",
    "              validation_data=(val_X, val_y),\n",
    "              batch_size=256, epochs=4, verbose=2,\n",
    "             callbacks = [lr, model_checkpoint])\n",
    "    print_step(\"Loading best model\")\n",
    "    model.load_weights(filepath = 'tmp_bst_mdl.hdf5')\n",
    "    print_step('Predict Val 1/2')\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    print_step('Predict Test 2/2')\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return pred_val_y, pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-26 22:26:12.572093] Started  fold 1/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/4\n",
      " - 23s - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 2/4\n",
      " - 22s - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 3/4\n",
      " - 22s - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 4/4\n",
      " - 23s - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "[2018-06-26 22:27:43.245957] Loading best model\n",
      "[2018-06-26 22:27:43.271763] Predict Val 1/2\n",
      "[2018-06-26 22:28:02.184540] Predict Test 2/2\n",
      "[2018-06-26 22:28:35.242447]  cv score 1 : 0.21084402971692776\n",
      "[2018-06-26 22:28:35.251862] Started  fold 2/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/4\n",
      " - 23s - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 2/4\n",
      " - 22s - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 3/4\n",
      " - 22s - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 4/4\n",
      " - 22s - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "[2018-06-26 22:30:05.515341] Loading best model\n",
      "[2018-06-26 22:30:05.539614] Predict Val 1/2\n",
      "[2018-06-26 22:30:24.663036] Predict Test 2/2\n",
      "[2018-06-26 22:30:56.957469]  cv score 2 : 0.2099979237240734\n",
      "[2018-06-26 22:30:56.968416] Started  fold 3/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/4\n",
      " - 24s - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 2/4\n",
      " - 23s - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 3/4\n",
      " - 22s - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 4/4\n",
      " - 22s - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "[2018-06-26 22:32:28.372507] Loading best model\n",
      "[2018-06-26 22:32:28.396807] Predict Val 1/2\n",
      "[2018-06-26 22:32:47.616446] Predict Test 2/2\n",
      "[2018-06-26 22:33:19.689222]  cv score 3 : 0.20989231103783748\n",
      "[2018-06-26 22:33:19.698727] Started  fold 4/5\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/4\n",
      " - 22s - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 2/4\n",
      " - 23s - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 3/4\n",
      " - 22s - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 4/4\n",
      " - 22s - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "[2018-06-26 22:34:50.136335] Loading best model\n",
      "[2018-06-26 22:34:50.160794] Predict Val 1/2\n",
      "[2018-06-26 22:35:09.242324] Predict Test 2/2\n",
      "[2018-06-26 22:35:41.965500]  cv score 4 : 0.20973659925705487\n",
      "[2018-06-26 22:35:41.976188] Started  fold 5/5\n",
      "Train on 1202740 samples, validate on 300684 samples\n",
      "Epoch 1/4\n",
      " - 23s - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 2/4\n",
      " - 23s - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 3/4\n",
      " - 22s - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 4/4\n",
      " - 22s - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "[2018-06-26 22:37:11.903997] Loading best model\n",
      "[2018-06-26 22:37:11.928356] Predict Val 1/2\n",
      "[2018-06-26 22:37:31.595624] Predict Test 2/2\n",
      "[2018-06-26 22:38:03.856279]  cv score 5 : 0.21036616964768748\n",
      "[2018-06-26 22:38:03.856471]  cv scores : [0.21084402971692776, 0.2099979237240734, 0.20989231103783748, 0.20973659925705487, 0.21036616964768748]\n",
      "[2018-06-26 22:38:03.856528]  mean cv score : 0.2101674066767162\n",
      "[2018-06-26 22:38:03.856695]  std cv score : 0.0003968179864311406\n"
     ]
    }
   ],
   "source": [
    "results = run_cv_model(train_, test_, target, runMLP, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~\n",
      "[2018-06-26 22:41:40.469998] Cache\n",
      "[2018-06-26 22:41:43.209133] Saved cache/train_MLP_blender.csv and cache/test_MLP_blender.csv to cache!\n"
     ]
    }
   ],
   "source": [
    "print('~~~~~~~~~~')\n",
    "print_step('Cache')\n",
    "save_in_cache('MLP_blender', pd.DataFrame({'MLP_blender': results['train']}),\n",
    "                           pd.DataFrame({'MLP_blender': results['test'].flatten()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[2018-06-19 22:50:04.882865] Prepping submission file\n",
      "[2018-06-19 22:50:06.778460] Done!\n"
     ]
    }
   ],
   "source": [
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print_step('Prepping submission file')\n",
    "submission = pd.read_csv(f'data/test.csv', usecols=['item_id'])\n",
    "submission['deal_probability'] = results['test'].clip(0.0, 1.0)\n",
    "submission.to_csv('submit/submit_MLP_blender.csv', index=False)\n",
    "print_step('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
